{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 转换并量化中文LLaMA/Alpaca模型\n",
        "注意：由于最小的7B模型转换也需要13G以上可用内存，**如果没有Colab Pro及更高订阅是无法完成转换的**。不过仍然可以参考整个流程，以便在其他机器上运行并对照。\n",
        "\n",
        "运行前，请选择 “代码执行程序” -> “更改运行时类型” -> “高RAM”"
      ],
      "metadata": {
        "id": "B1c96_k3MahN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 安装相关依赖"
      ],
      "metadata": {
        "id": "vScqHD_jMFOV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5WKFJXIL6ZU",
        "outputId": "00762b43-55ee-4432-82e1-1b5e55fca4f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-p0m59mzk\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-p0m59mzk\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit c612628045822f909020f7eb6784c79700813eda\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (3.10.7)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (1.26.15)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.28.0.dev0-py3-none-any.whl size=6858427 sha256=86aff753c22c612d02464bac58c26c3679b610b8c01aa2764e5d6b48361f3eef\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yjpu8f0y/wheels/f7/92/8c/752ff3bfcd3439805d8bbf641614da38ef3226e127ebea86ee\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.28.0.dev0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting peft\n",
            "  Downloading peft-0.2.0-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from peft) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from peft) (23.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (from peft) (4.28.0.dev0)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from peft) (5.9.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from peft) (6.0)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.9/dist-packages (from peft) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.13.0->peft) (4.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (3.10.7)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers->peft) (0.13.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->peft) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->peft) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->peft) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->peft) (3.4)\n",
            "Installing collected packages: accelerate, peft\n",
            "Successfully installed accelerate-0.18.0 peft-0.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install peft\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 克隆目录和代码"
      ],
      "metadata": {
        "id": "ygb1xFIMNQKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ymcui/Chinese-LLaMA-Alpaca\n",
        "!git clone https://github.com/ggerganov/llama.cpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCEJh7NJNXz9",
        "outputId": "2b33ba8a-8d57-4872-bbd2-282a52293834"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Chinese-LLaMA-Alpaca'...\n",
            "remote: Enumerating objects: 242, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/92)\u001b[K\rremote: Counting objects:   2% (2/92)\u001b[K\rremote: Counting objects:   3% (3/92)\u001b[K\rremote: Counting objects:   4% (4/92)\u001b[K\rremote: Counting objects:   5% (5/92)\u001b[K\rremote: Counting objects:   6% (6/92)\u001b[K\rremote: Counting objects:   7% (7/92)\u001b[K\rremote: Counting objects:   8% (8/92)\u001b[K\rremote: Counting objects:   9% (9/92)\u001b[K\rremote: Counting objects:  10% (10/92)\u001b[K\rremote: Counting objects:  11% (11/92)\u001b[K\rremote: Counting objects:  13% (12/92)\u001b[K\rremote: Counting objects:  14% (13/92)\u001b[K\rremote: Counting objects:  15% (14/92)\u001b[K\rremote: Counting objects:  16% (15/92)\u001b[K\rremote: Counting objects:  17% (16/92)\u001b[K\rremote: Counting objects:  18% (17/92)\u001b[K\rremote: Counting objects:  19% (18/92)\u001b[K\rremote: Counting objects:  20% (19/92)\u001b[K\rremote: Counting objects:  21% (20/92)\u001b[K\rremote: Counting objects:  22% (21/92)\u001b[K\rremote: Counting objects:  23% (22/92)\u001b[K\rremote: Counting objects:  25% (23/92)\u001b[K\rremote: Counting objects:  26% (24/92)\u001b[K\rremote: Counting objects:  27% (25/92)\u001b[K\rremote: Counting objects:  28% (26/92)\u001b[K\rremote: Counting objects:  29% (27/92)\u001b[K\rremote: Counting objects:  30% (28/92)\u001b[K\rremote: Counting objects:  31% (29/92)\u001b[K\rremote: Counting objects:  32% (30/92)\u001b[K\rremote: Counting objects:  33% (31/92)\u001b[K\rremote: Counting objects:  34% (32/92)\u001b[K\rremote: Counting objects:  35% (33/92)\u001b[K\rremote: Counting objects:  36% (34/92)\u001b[K\rremote: Counting objects:  38% (35/92)\u001b[K\rremote: Counting objects:  39% (36/92)\u001b[K\rremote: Counting objects:  40% (37/92)\u001b[K\rremote: Counting objects:  41% (38/92)\u001b[K\rremote: Counting objects:  42% (39/92)\u001b[K\rremote: Counting objects:  43% (40/92)\u001b[K\rremote: Counting objects:  44% (41/92)\u001b[K\rremote: Counting objects:  45% (42/92)\u001b[K\rremote: Counting objects:  46% (43/92)\u001b[K\rremote: Counting objects:  47% (44/92)\u001b[K\rremote: Counting objects:  48% (45/92)\u001b[K\rremote: Counting objects:  50% (46/92)\u001b[K\rremote: Counting objects:  51% (47/92)\u001b[K\rremote: Counting objects:  52% (48/92)\u001b[K\rremote: Counting objects:  53% (49/92)\u001b[K\rremote: Counting objects:  54% (50/92)\u001b[K\rremote: Counting objects:  55% (51/92)\u001b[K\rremote: Counting objects:  56% (52/92)\u001b[K\rremote: Counting objects:  57% (53/92)\u001b[K\rremote: Counting objects:  58% (54/92)\u001b[K\rremote: Counting objects:  59% (55/92)\u001b[K\rremote: Counting objects:  60% (56/92)\u001b[K\rremote: Counting objects:  61% (57/92)\u001b[K\rremote: Counting objects:  63% (58/92)\u001b[K\rremote: Counting objects:  64% (59/92)\u001b[K\rremote: Counting objects:  65% (60/92)\u001b[K\rremote: Counting objects:  66% (61/92)\u001b[K\rremote: Counting objects:  67% (62/92)\u001b[K\rremote: Counting objects:  68% (63/92)\u001b[K\rremote: Counting objects:  69% (64/92)\u001b[K\rremote: Counting objects:  70% (65/92)\u001b[K\rremote: Counting objects:  71% (66/92)\u001b[K\rremote: Counting objects:  72% (67/92)\u001b[K\rremote: Counting objects:  73% (68/92)\u001b[K\rremote: Counting objects:  75% (69/92)\u001b[K\rremote: Counting objects:  76% (70/92)\u001b[K\rremote: Counting objects:  77% (71/92)\u001b[K\rremote: Counting objects:  78% (72/92)\u001b[K\rremote: Counting objects:  79% (73/92)\u001b[K\rremote: Counting objects:  80% (74/92)\u001b[K\rremote: Counting objects:  81% (75/92)\u001b[K\rremote: Counting objects:  82% (76/92)\u001b[K\rremote: Counting objects:  83% (77/92)\u001b[K\rremote: Counting objects:  84% (78/92)\u001b[K\rremote: Counting objects:  85% (79/92)\u001b[K\rremote: Counting objects:  86% (80/92)\u001b[K\rremote: Counting objects:  88% (81/92)\u001b[K\rremote: Counting objects:  89% (82/92)\u001b[K\rremote: Counting objects:  90% (83/92)\u001b[K\rremote: Counting objects:  91% (84/92)\u001b[K\rremote: Counting objects:  92% (85/92)\u001b[K\rremote: Counting objects:  93% (86/92)\u001b[K\rremote: Counting objects:  94% (87/92)\u001b[K\rremote: Counting objects:  95% (88/92)\u001b[K\rremote: Counting objects:  96% (89/92)\u001b[K\rremote: Counting objects:  97% (90/92)\u001b[K\rremote: Counting objects:  98% (91/92)\u001b[K\rremote: Counting objects: 100% (92/92)\u001b[K\rremote: Counting objects: 100% (92/92), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 242 (delta 34), reused 43 (delta 22), pack-reused 150\u001b[K\n",
            "Receiving objects: 100% (242/242), 9.71 MiB | 28.09 MiB/s, done.\n",
            "Resolving deltas: 100% (135/135), done.\n",
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 1336, done.\u001b[K\n",
            "remote: Counting objects: 100% (609/609), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 1336 (delta 562), reused 542 (delta 538), pack-reused 727\u001b[K\n",
            "Receiving objects: 100% (1336/1336), 921.16 KiB | 6.49 MiB/s, done.\n",
            "Resolving deltas: 100% (835/835), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 合并模型（以Alpaca-7B为例）\n",
        "注意，此处使用的是huggingface提供的基模型（已是HF格式），而不是facebook官方的LLaMA模型，因此这里略去将原版LLaMA转换为HF格式的步骤。\n",
        "\n",
        "直接运行第二步：合并LoRA权重，生成全量模型权重。可以直接指定🤗模型库的地址（也可以是本地存放地址）。\n",
        "- 基模型：`decapoda-research/llama-7b-hf`\n",
        "- LoRA模型：`ziqingyang/chinese-alpaca-lora-7b`\n",
        "\n",
        "该过程比较耗时，需要几分钟，请耐心等待。\n",
        "转换好的模型存放在`7B-combined`目录。\n",
        "如果你不需要量化模型，那么到这一步就结束了。"
      ],
      "metadata": {
        "id": "nIyxX0DSNsgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python ./Chinese-LLaMA-Alpaca/scripts/merge_llama_with_chinese_lora.py \\\n",
        "    --base_model 'decapoda-research/llama-7b-hf' \\\n",
        "    --lora_model 'ziqingyang/chinese-alpaca-lora-7b' \\\n",
        "    --output_dir 7B-combined"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AV4EW5hNhVV",
        "outputId": "08178033-5628-4471-9c5f-73a8bdf18175"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading tokenizer.model: 100% 758k/758k [00:00<00:00, 6.00MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 96.0/96.0 [00:00<00:00, 15.2kB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 166/166 [00:00<00:00, 62.5kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 427/427 [00:00<00:00, 57.2kB/s]\n",
            "Downloading (…)model.bin.index.json: 100% 25.5k/25.5k [00:00<00:00, 1.37MB/s]\n",
            "Downloading shards:   0% 0/33 [00:00<?, ?it/s]\n",
            "Downloading (…)l-00001-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:   3% 10.5M/405M [00:00<00:33, 11.7MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:   5% 21.0M/405M [00:01<00:19, 19.8MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:   8% 31.5M/405M [00:01<00:12, 28.9MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  10% 41.9M/405M [00:01<00:09, 39.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  16% 62.9M/405M [00:01<00:05, 60.3MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  21% 83.9M/405M [00:01<00:04, 79.1MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  26% 105M/405M [00:01<00:03, 93.9MB/s] \u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  31% 126M/405M [00:02<00:02, 105MB/s] \u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  36% 147M/405M [00:02<00:02, 114MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  41% 168M/405M [00:02<00:01, 120MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  47% 189M/405M [00:02<00:01, 124MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  52% 210M/405M [00:02<00:01, 127MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  57% 231M/405M [00:02<00:01, 129MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  62% 252M/405M [00:03<00:01, 130MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  67% 273M/405M [00:03<00:01, 130MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  73% 294M/405M [00:03<00:00, 131MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  78% 315M/405M [00:03<00:00, 133MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  83% 336M/405M [00:03<00:00, 120MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  88% 357M/405M [00:03<00:00, 124MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin:  93% 377M/405M [00:04<00:00, 126MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00033.bin: 100% 405M/405M [00:04<00:00, 95.0MB/s]\n",
            "Downloading shards:   3% 1/33 [00:04<02:26,  4.58s/it]\n",
            "Downloading (…)l-00002-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:   3% 10.5M/405M [00:00<00:27, 14.4MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:   5% 21.0M/405M [00:00<00:16, 23.1MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:   8% 31.5M/405M [00:01<00:11, 32.6MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  10% 41.9M/405M [00:01<00:08, 43.0MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  16% 62.9M/405M [00:01<00:05, 64.2MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  21% 83.9M/405M [00:01<00:04, 78.6MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  26% 105M/405M [00:01<00:03, 86.8MB/s] \u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  31% 126M/405M [00:02<00:02, 97.3MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  36% 147M/405M [00:02<00:02, 106MB/s] \u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  41% 168M/405M [00:02<00:02, 112MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  47% 189M/405M [00:02<00:01, 117MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  52% 210M/405M [00:02<00:01, 120MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  57% 231M/405M [00:02<00:01, 122MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  62% 252M/405M [00:02<00:01, 125MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  67% 273M/405M [00:03<00:01, 127MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  73% 294M/405M [00:03<00:00, 128MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  78% 315M/405M [00:03<00:00, 129MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  83% 336M/405M [00:03<00:00, 124MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  88% 357M/405M [00:03<00:00, 125MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin:  93% 377M/405M [00:03<00:00, 127MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00033.bin: 100% 405M/405M [00:04<00:00, 96.0MB/s]\n",
            "Downloading shards:   6% 2/33 [00:09<02:21,  4.55s/it]\n",
            "Downloading (…)l-00003-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:   3% 10.5M/405M [00:00<00:34, 11.6MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:   5% 21.0M/405M [00:01<00:19, 19.6MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:   8% 31.5M/405M [00:01<00:13, 28.5MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  10% 41.9M/405M [00:01<00:09, 38.7MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  16% 62.9M/405M [00:01<00:05, 59.6MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  21% 83.9M/405M [00:01<00:04, 78.3MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  26% 105M/405M [00:01<00:03, 92.9MB/s] \u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  31% 126M/405M [00:02<00:02, 104MB/s] \u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  36% 147M/405M [00:02<00:02, 109MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  41% 168M/405M [00:02<00:02, 116MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  47% 189M/405M [00:02<00:01, 121MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  52% 210M/405M [00:02<00:01, 123MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  57% 231M/405M [00:02<00:01, 126MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  62% 252M/405M [00:03<00:01, 128MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  67% 273M/405M [00:03<00:01, 129MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  73% 294M/405M [00:03<00:00, 130MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  78% 315M/405M [00:03<00:00, 131MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  83% 336M/405M [00:03<00:00, 132MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  88% 357M/405M [00:03<00:00, 130MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin:  93% 377M/405M [00:04<00:00, 132MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00033.bin: 100% 405M/405M [00:04<00:00, 94.7MB/s]\n",
            "Downloading shards:   9% 3/33 [00:13<02:20,  4.67s/it]\n",
            "Downloading (…)l-00004-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:   3% 10.5M/405M [00:00<00:33, 11.7MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:   5% 21.0M/405M [00:01<00:19, 19.7MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:   8% 31.5M/405M [00:01<00:13, 28.6MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  10% 41.9M/405M [00:01<00:09, 38.8MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  16% 62.9M/405M [00:01<00:05, 59.7MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  21% 83.9M/405M [00:01<00:04, 78.3MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  26% 105M/405M [00:01<00:03, 93.0MB/s] \u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  31% 126M/405M [00:02<00:02, 104MB/s] \u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  36% 147M/405M [00:02<00:02, 107MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  41% 168M/405M [00:02<00:02, 113MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  47% 189M/405M [00:02<00:01, 119MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  52% 210M/405M [00:02<00:01, 123MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  57% 231M/405M [00:02<00:01, 125MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  62% 252M/405M [00:03<00:01, 128MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  67% 273M/405M [00:03<00:01, 129MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  73% 294M/405M [00:03<00:00, 131MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  78% 315M/405M [00:03<00:00, 131MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  83% 336M/405M [00:03<00:00, 132MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  88% 357M/405M [00:03<00:00, 130MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin:  93% 377M/405M [00:04<00:00, 124MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00033.bin: 100% 405M/405M [00:04<00:00, 94.0MB/s]\n",
            "Downloading shards:  12% 4/33 [00:18<02:14,  4.65s/it]\n",
            "Downloading (…)l-00005-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:   3% 10.5M/405M [00:00<00:34, 11.6MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:   5% 21.0M/405M [00:01<00:19, 19.5MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:   8% 31.5M/405M [00:01<00:13, 28.4MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  10% 41.9M/405M [00:01<00:09, 38.4MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  13% 52.4M/405M [00:01<00:07, 49.6MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  18% 73.4M/405M [00:01<00:04, 72.1MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  23% 94.4M/405M [00:01<00:03, 89.1MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  28% 115M/405M [00:02<00:02, 100MB/s]  \u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  34% 136M/405M [00:02<00:02, 110MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  39% 157M/405M [00:02<00:02, 116MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  44% 178M/405M [00:02<00:01, 121MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  49% 199M/405M [00:02<00:01, 124MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  54% 220M/405M [00:02<00:01, 127MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  60% 241M/405M [00:03<00:01, 122MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  65% 262M/405M [00:03<00:01, 124MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  70% 283M/405M [00:03<00:00, 127MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  75% 304M/405M [00:03<00:00, 128MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  80% 325M/405M [00:03<00:00, 130MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  85% 346M/405M [00:03<00:00, 130MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  91% 367M/405M [00:04<00:00, 131MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin:  96% 388M/405M [00:04<00:00, 128MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00033.bin: 100% 405M/405M [00:04<00:00, 93.7MB/s]\n",
            "Downloading shards:  15% 5/33 [00:23<02:10,  4.64s/it]\n",
            "Downloading (…)l-00006-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:   3% 10.5M/405M [00:00<00:28, 13.9MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:   5% 21.0M/405M [00:01<00:16, 22.6MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:   8% 31.5M/405M [00:01<00:11, 32.0MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  10% 41.9M/405M [00:01<00:08, 41.6MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  16% 62.9M/405M [00:01<00:05, 62.9MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  21% 83.9M/405M [00:01<00:04, 79.9MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  26% 105M/405M [00:01<00:03, 93.0MB/s] \u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  31% 126M/405M [00:01<00:02, 104MB/s] \u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  36% 147M/405M [00:02<00:02, 113MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  41% 168M/405M [00:02<00:02, 118MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  47% 189M/405M [00:02<00:01, 122MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  52% 210M/405M [00:02<00:01, 126MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  57% 231M/405M [00:02<00:01, 127MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  62% 252M/405M [00:02<00:01, 128MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  67% 273M/405M [00:03<00:01, 125MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  73% 294M/405M [00:03<00:00, 127MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  78% 315M/405M [00:03<00:00, 128MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  83% 336M/405M [00:03<00:00, 128MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  88% 357M/405M [00:03<00:00, 128MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin:  93% 377M/405M [00:03<00:00, 130MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00033.bin: 100% 405M/405M [00:04<00:00, 97.7MB/s]\n",
            "Downloading shards:  18% 6/33 [00:27<02:03,  4.58s/it]\n",
            "Downloading (…)l-00007-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:   3% 10.5M/405M [00:00<00:34, 11.6MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:   5% 21.0M/405M [00:01<00:19, 19.6MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:   8% 31.5M/405M [00:01<00:13, 28.5MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  10% 41.9M/405M [00:01<00:09, 38.7MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  13% 52.4M/405M [00:01<00:07, 50.1MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  18% 73.4M/405M [00:01<00:04, 72.7MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  23% 94.4M/405M [00:01<00:03, 88.4MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  28% 115M/405M [00:02<00:02, 101MB/s]  \u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  34% 136M/405M [00:02<00:02, 111MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  39% 157M/405M [00:02<00:02, 118MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  44% 178M/405M [00:02<00:01, 123MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  49% 199M/405M [00:02<00:01, 125MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  54% 220M/405M [00:02<00:01, 127MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  60% 241M/405M [00:03<00:01, 129MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  65% 262M/405M [00:03<00:01, 130MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  70% 283M/405M [00:03<00:00, 129MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  75% 304M/405M [00:03<00:00, 131MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  80% 325M/405M [00:03<00:00, 132MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  85% 346M/405M [00:03<00:00, 130MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  91% 367M/405M [00:03<00:00, 128MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin:  96% 388M/405M [00:04<00:00, 130MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00033.bin: 100% 405M/405M [00:04<00:00, 94.5MB/s]\n",
            "Downloading shards:  21% 7/33 [00:32<01:59,  4.59s/it]\n",
            "Downloading (…)l-00008-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:   3% 10.5M/405M [00:00<00:27, 14.3MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:   5% 21.0M/405M [00:00<00:16, 23.0MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:   8% 31.5M/405M [00:01<00:11, 32.5MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  10% 41.9M/405M [00:01<00:08, 43.0MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  16% 62.9M/405M [00:01<00:05, 64.3MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  21% 83.9M/405M [00:01<00:03, 82.2MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  26% 105M/405M [00:01<00:03, 96.1MB/s] \u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  31% 126M/405M [00:01<00:02, 102MB/s] \u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  36% 147M/405M [00:02<00:02, 101MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  41% 168M/405M [00:02<00:02, 102MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  47% 189M/405M [00:02<00:02, 103MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  52% 210M/405M [00:02<00:01, 104MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  57% 231M/405M [00:02<00:01, 105MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  62% 252M/405M [00:03<00:01, 107MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  67% 273M/405M [00:03<00:01, 106MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  73% 294M/405M [00:03<00:01, 108MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  78% 315M/405M [00:03<00:00, 104MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  83% 336M/405M [00:03<00:00, 106MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  88% 357M/405M [00:04<00:00, 105MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin:  93% 377M/405M [00:04<00:00, 105MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00033.bin: 100% 405M/405M [00:04<00:00, 86.8MB/s]\n",
            "Downloading shards:  24% 8/33 [00:37<01:58,  4.73s/it]\n",
            "Downloading (…)l-00009-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:   3% 10.5M/405M [00:00<00:20, 19.2MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:   5% 21.0M/405M [00:00<00:13, 28.2MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:   8% 31.5M/405M [00:00<00:09, 38.3MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  10% 41.9M/405M [00:01<00:07, 48.0MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  13% 52.4M/405M [00:01<00:06, 54.7MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  16% 62.9M/405M [00:01<00:05, 63.2MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  21% 83.9M/405M [00:01<00:04, 79.5MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  23% 94.4M/405M [00:01<00:03, 84.7MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  26% 105M/405M [00:01<00:03, 89.2MB/s] \u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  28% 115M/405M [00:01<00:03, 92.6MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  31% 126M/405M [00:01<00:02, 94.2MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  34% 136M/405M [00:02<00:02, 95.5MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  39% 157M/405M [00:02<00:02, 94.2MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  44% 178M/405M [00:02<00:02, 101MB/s] \u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  47% 189M/405M [00:02<00:02, 102MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  49% 199M/405M [00:02<00:02, 102MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  52% 210M/405M [00:02<00:01, 102MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  57% 231M/405M [00:02<00:01, 103MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  62% 252M/405M [00:03<00:01, 104MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  67% 273M/405M [00:03<00:01, 104MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  70% 283M/405M [00:03<00:01, 103MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  73% 294M/405M [00:03<00:01, 101MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  75% 304M/405M [00:03<00:01, 100MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  80% 325M/405M [00:03<00:00, 102MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  83% 336M/405M [00:04<00:00, 101MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  85% 346M/405M [00:04<00:00, 102MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  88% 357M/405M [00:04<00:00, 102MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  91% 367M/405M [00:04<00:00, 102MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin:  96% 388M/405M [00:04<00:00, 104MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00033.bin: 100% 405M/405M [00:04<00:00, 86.2MB/s]\n",
            "Downloading shards:  27% 9/33 [00:42<01:55,  4.82s/it]\n",
            "Downloading (…)l-00010-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:   3% 10.5M/405M [00:00<00:28, 13.9MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:   5% 21.0M/405M [00:01<00:17, 21.8MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:   8% 31.5M/405M [00:01<00:12, 30.5MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  10% 41.9M/405M [00:01<00:08, 40.7MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  13% 52.4M/405M [00:01<00:06, 50.5MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  18% 73.4M/405M [00:01<00:04, 68.1MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  21% 83.9M/405M [00:01<00:04, 75.0MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  23% 94.4M/405M [00:01<00:03, 81.1MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  26% 105M/405M [00:01<00:03, 84.7MB/s] \u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  28% 115M/405M [00:02<00:03, 87.7MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  31% 126M/405M [00:02<00:03, 90.1MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  36% 147M/405M [00:02<00:02, 97.0MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  41% 168M/405M [00:03<00:07, 29.8MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  44% 178M/405M [00:03<00:06, 34.9MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  47% 189M/405M [00:04<00:05, 40.9MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  49% 199M/405M [00:04<00:04, 48.3MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  52% 210M/405M [00:04<00:03, 56.0MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  54% 220M/405M [00:04<00:02, 63.6MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  60% 241M/405M [00:04<00:02, 77.6MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  62% 252M/405M [00:04<00:01, 81.7MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  67% 273M/405M [00:04<00:01, 90.5MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  73% 294M/405M [00:05<00:01, 94.9MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  75% 304M/405M [00:05<00:01, 95.3MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  80% 325M/405M [00:05<00:00, 100MB/s] \u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  85% 346M/405M [00:05<00:00, 104MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  91% 367M/405M [00:05<00:00, 105MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin:  96% 388M/405M [00:05<00:00, 107MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00033.bin: 100% 405M/405M [00:06<00:00, 65.9MB/s]\n",
            "Downloading shards:  30% 10/33 [00:48<02:02,  5.34s/it]\n",
            "Downloading (…)l-00011-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:   3% 10.5M/405M [00:00<00:19, 20.3MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:   5% 21.0M/405M [00:00<00:13, 29.1MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:   8% 31.5M/405M [00:00<00:09, 39.7MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  10% 41.9M/405M [00:01<00:07, 48.7MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  13% 52.4M/405M [00:01<00:06, 57.3MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  16% 62.9M/405M [00:01<00:05, 65.4MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  18% 73.4M/405M [00:01<00:04, 73.7MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  21% 83.9M/405M [00:01<00:04, 79.7MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  23% 94.4M/405M [00:01<00:03, 80.2MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  26% 105M/405M [00:01<00:03, 81.3MB/s] \u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  28% 115M/405M [00:01<00:03, 86.5MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  31% 126M/405M [00:01<00:03, 90.1MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  34% 136M/405M [00:02<00:02, 91.2MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  36% 147M/405M [00:02<00:02, 93.2MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  39% 157M/405M [00:02<00:02, 94.9MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  44% 178M/405M [00:02<00:02, 102MB/s] \u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  49% 199M/405M [00:02<00:01, 107MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  54% 220M/405M [00:02<00:01, 110MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  60% 241M/405M [00:03<00:01, 112MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  65% 262M/405M [00:03<00:01, 109MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  70% 283M/405M [00:03<00:01, 111MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  75% 304M/405M [00:03<00:00, 113MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  80% 325M/405M [00:03<00:00, 114MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  85% 346M/405M [00:03<00:00, 115MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  91% 367M/405M [00:04<00:00, 115MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin:  96% 388M/405M [00:04<00:00, 116MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00033.bin: 100% 405M/405M [00:04<00:00, 90.2MB/s]\n",
            "Downloading shards:  33% 11/33 [00:53<01:53,  5.18s/it]\n",
            "Downloading (…)l-00012-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:   3% 10.5M/405M [00:00<00:22, 17.3MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:   5% 21.0M/405M [00:00<00:15, 25.4MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:   8% 31.5M/405M [00:01<00:10, 34.8MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  10% 41.9M/405M [00:01<00:08, 45.1MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  13% 52.4M/405M [00:01<00:06, 55.9MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  16% 62.9M/405M [00:01<00:05, 66.0MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  21% 83.9M/405M [00:01<00:03, 84.8MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  26% 105M/405M [00:01<00:03, 96.5MB/s] \u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  31% 126M/405M [00:01<00:02, 104MB/s] \u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  36% 147M/405M [00:02<00:02, 109MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  41% 168M/405M [00:02<00:02, 108MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  47% 189M/405M [00:02<00:01, 110MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  52% 210M/405M [00:02<00:01, 112MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  57% 231M/405M [00:02<00:01, 113MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  62% 252M/405M [00:03<00:01, 114MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  67% 273M/405M [00:03<00:01, 114MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  73% 294M/405M [00:03<00:00, 115MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  78% 315M/405M [00:03<00:00, 114MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  83% 336M/405M [00:03<00:00, 114MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  88% 357M/405M [00:03<00:00, 112MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin:  93% 377M/405M [00:04<00:00, 112MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00033.bin: 100% 405M/405M [00:04<00:00, 92.1MB/s]\n",
            "Downloading shards:  36% 12/33 [00:58<01:45,  5.04s/it]\n",
            "Downloading (…)l-00013-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:   3% 10.5M/405M [00:00<00:12, 30.9MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:   5% 21.0M/405M [00:00<00:09, 41.5MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:   8% 31.5M/405M [00:00<00:07, 52.7MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  10% 41.9M/405M [00:00<00:05, 63.5MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  13% 52.4M/405M [00:00<00:04, 72.9MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  18% 73.4M/405M [00:01<00:03, 90.2MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  23% 94.4M/405M [00:01<00:03, 99.7MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  28% 115M/405M [00:01<00:02, 106MB/s]  \u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  34% 136M/405M [00:01<00:02, 108MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  39% 157M/405M [00:01<00:02, 111MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  44% 178M/405M [00:01<00:02, 112MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  49% 199M/405M [00:02<00:01, 113MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  54% 220M/405M [00:02<00:01, 116MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  60% 241M/405M [00:02<00:01, 113MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  65% 262M/405M [00:02<00:01, 108MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  70% 283M/405M [00:02<00:01, 108MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  75% 304M/405M [00:03<00:00, 110MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  80% 325M/405M [00:03<00:00, 113MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  85% 346M/405M [00:03<00:00, 114MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  91% 367M/405M [00:03<00:00, 115MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin:  96% 388M/405M [00:03<00:00, 115MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00033.bin: 100% 405M/405M [00:03<00:00, 101MB/s]\n",
            "Downloading shards:  39% 13/33 [01:02<01:36,  4.82s/it]\n",
            "Downloading (…)l-00014-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:   3% 10.5M/405M [00:00<00:22, 17.5MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:   5% 21.0M/405M [00:00<00:14, 26.3MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:   8% 31.5M/405M [00:01<00:10, 36.0MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  10% 41.9M/405M [00:01<00:07, 46.6MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  13% 52.4M/405M [00:01<00:06, 57.7MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  16% 62.9M/405M [00:01<00:05, 66.8MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  21% 83.9M/405M [00:01<00:03, 80.7MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  26% 105M/405M [00:01<00:03, 92.2MB/s] \u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  31% 126M/405M [00:01<00:02, 99.6MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  36% 147M/405M [00:02<00:02, 104MB/s] \u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  41% 168M/405M [00:02<00:02, 108MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  47% 189M/405M [00:02<00:01, 110MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  52% 210M/405M [00:02<00:01, 113MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  57% 231M/405M [00:02<00:01, 115MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  62% 252M/405M [00:03<00:01, 116MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  67% 273M/405M [00:03<00:01, 116MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  73% 294M/405M [00:03<00:00, 116MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  78% 315M/405M [00:03<00:00, 116MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  83% 336M/405M [00:03<00:00, 117MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  88% 357M/405M [00:03<00:00, 115MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin:  93% 377M/405M [00:04<00:00, 114MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00033.bin: 100% 405M/405M [00:04<00:00, 92.9MB/s]\n",
            "Downloading shards:  42% 14/33 [01:07<01:31,  4.79s/it]\n",
            "Downloading (…)l-00015-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:   3% 10.5M/405M [00:00<00:33, 11.6MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:   5% 21.0M/405M [00:01<00:19, 19.5MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:   8% 31.5M/405M [00:01<00:13, 28.2MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  10% 41.9M/405M [00:01<00:09, 37.7MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  13% 52.4M/405M [00:01<00:07, 47.8MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  16% 62.9M/405M [00:01<00:05, 58.1MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  21% 83.9M/405M [00:01<00:04, 77.3MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  26% 105M/405M [00:02<00:03, 90.1MB/s] \u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  31% 126M/405M [00:02<00:02, 98.0MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  36% 147M/405M [00:02<00:02, 103MB/s] \u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  41% 168M/405M [00:02<00:02, 103MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  47% 189M/405M [00:02<00:01, 108MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  52% 210M/405M [00:02<00:01, 113MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  57% 231M/405M [00:03<00:01, 115MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  62% 252M/405M [00:03<00:01, 110MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  67% 273M/405M [00:03<00:01, 112MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  73% 294M/405M [00:03<00:00, 113MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  78% 315M/405M [00:03<00:00, 114MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  83% 336M/405M [00:04<00:00, 116MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  88% 357M/405M [00:04<00:00, 117MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin:  93% 377M/405M [00:04<00:00, 120MB/s]\u001b[A\n",
            "Downloading (…)l-00015-of-00033.bin: 100% 405M/405M [00:04<00:00, 86.8MB/s]\n",
            "Downloading shards:  45% 15/33 [01:12<01:27,  4.87s/it]\n",
            "Downloading (…)l-00016-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:   3% 10.5M/405M [00:00<00:24, 15.8MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:   5% 21.0M/405M [00:00<00:15, 24.6MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:   8% 31.5M/405M [00:01<00:11, 33.9MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  10% 41.9M/405M [00:01<00:08, 44.0MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  13% 52.4M/405M [00:01<00:06, 54.9MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  18% 73.4M/405M [00:01<00:04, 75.6MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  23% 94.4M/405M [00:01<00:03, 88.9MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  28% 115M/405M [00:01<00:02, 98.6MB/s] \u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  34% 136M/405M [00:02<00:02, 103MB/s] \u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  39% 157M/405M [00:02<00:02, 106MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  44% 178M/405M [00:02<00:02, 108MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  49% 199M/405M [00:02<00:01, 107MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  54% 220M/405M [00:02<00:01, 109MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  60% 241M/405M [00:02<00:01, 112MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  65% 262M/405M [00:03<00:01, 113MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  70% 283M/405M [00:03<00:01, 114MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  75% 304M/405M [00:03<00:00, 115MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  80% 325M/405M [00:03<00:00, 116MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  85% 346M/405M [00:03<00:00, 116MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  91% 367M/405M [00:04<00:00, 115MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin:  96% 388M/405M [00:04<00:00, 116MB/s]\u001b[A\n",
            "Downloading (…)l-00016-of-00033.bin: 100% 405M/405M [00:04<00:00, 91.8MB/s]\n",
            "Downloading shards:  48% 16/33 [01:17<01:22,  4.83s/it]\n",
            "Downloading (…)l-00017-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:   3% 10.5M/405M [00:00<00:28, 13.9MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:   5% 21.0M/405M [00:01<00:17, 22.3MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:   8% 31.5M/405M [00:01<00:11, 31.3MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  10% 41.9M/405M [00:01<00:09, 40.2MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  13% 52.4M/405M [00:01<00:07, 49.9MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  16% 62.9M/405M [00:01<00:05, 59.1MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  21% 83.9M/405M [00:01<00:04, 77.5MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  26% 105M/405M [00:01<00:03, 91.5MB/s] \u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  31% 126M/405M [00:02<00:02, 102MB/s] \u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  36% 147M/405M [00:02<00:02, 106MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  41% 168M/405M [00:02<00:02, 110MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  47% 189M/405M [00:02<00:01, 114MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  52% 210M/405M [00:02<00:01, 115MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  57% 231M/405M [00:02<00:01, 116MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  62% 252M/405M [00:03<00:01, 115MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  67% 273M/405M [00:03<00:01, 113MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  73% 294M/405M [00:03<00:00, 114MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  78% 315M/405M [00:03<00:00, 115MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  83% 336M/405M [00:03<00:00, 116MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  88% 357M/405M [00:04<00:00, 117MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin:  93% 377M/405M [00:04<00:00, 119MB/s]\u001b[A\n",
            "Downloading (…)l-00017-of-00033.bin: 100% 405M/405M [00:04<00:00, 89.4MB/s]\n",
            "Downloading shards:  52% 17/33 [01:22<01:17,  4.84s/it]\n",
            "Downloading (…)l-00018-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:   3% 10.5M/405M [00:00<00:34, 11.4MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:   5% 21.0M/405M [00:01<00:20, 19.0MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:   8% 31.5M/405M [00:01<00:13, 27.4MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  10% 41.9M/405M [00:01<00:09, 36.9MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  13% 52.4M/405M [00:01<00:07, 47.3MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  16% 62.9M/405M [00:01<00:05, 57.7MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  21% 83.9M/405M [00:01<00:04, 77.4MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  26% 105M/405M [00:02<00:03, 90.8MB/s] \u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  31% 126M/405M [00:02<00:03, 92.5MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  34% 136M/405M [00:02<00:02, 91.2MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  39% 157M/405M [00:02<00:02, 98.9MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  44% 178M/405M [00:02<00:02, 105MB/s] \u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  49% 199M/405M [00:02<00:01, 109MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  54% 220M/405M [00:03<00:01, 112MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  60% 241M/405M [00:03<00:01, 114MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  65% 262M/405M [00:03<00:01, 111MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  70% 283M/405M [00:03<00:01, 108MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  75% 304M/405M [00:03<00:00, 107MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  80% 325M/405M [00:04<00:00, 110MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  85% 346M/405M [00:04<00:00, 112MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  91% 367M/405M [00:04<00:00, 116MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin:  96% 388M/405M [00:04<00:00, 117MB/s]\u001b[A\n",
            "Downloading (…)l-00018-of-00033.bin: 100% 405M/405M [00:04<00:00, 84.5MB/s]\n",
            "Downloading shards:  55% 18/33 [01:27<01:13,  4.92s/it]\n",
            "Downloading (…)l-00019-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:   3% 10.5M/405M [00:00<00:33, 11.6MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:   5% 21.0M/405M [00:01<00:19, 19.7MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:   8% 31.5M/405M [00:01<00:13, 28.3MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  10% 41.9M/405M [00:01<00:09, 37.9MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  16% 62.9M/405M [00:01<00:05, 58.5MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  21% 83.9M/405M [00:01<00:04, 75.1MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  26% 105M/405M [00:02<00:03, 88.7MB/s] \u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  31% 126M/405M [00:02<00:02, 102MB/s] \u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  36% 147M/405M [00:02<00:02, 105MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  41% 168M/405M [00:02<00:02, 104MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  47% 189M/405M [00:02<00:02, 108MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  52% 210M/405M [00:02<00:01, 110MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  57% 231M/405M [00:03<00:01, 112MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  62% 252M/405M [00:03<00:01, 114MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  67% 273M/405M [00:03<00:01, 116MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  73% 294M/405M [00:03<00:00, 118MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  78% 315M/405M [00:03<00:00, 119MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  83% 336M/405M [00:03<00:00, 120MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  88% 357M/405M [00:04<00:00, 119MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin:  93% 377M/405M [00:04<00:00, 117MB/s]\u001b[A\n",
            "Downloading (…)l-00019-of-00033.bin: 100% 405M/405M [00:04<00:00, 89.0MB/s]\n",
            "Downloading shards:  58% 19/33 [01:31<01:08,  4.90s/it]\n",
            "Downloading (…)l-00020-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:   3% 10.5M/405M [00:00<00:33, 11.6MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:   5% 21.0M/405M [00:01<00:19, 19.7MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:   8% 31.5M/405M [00:01<00:13, 28.7MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  10% 41.9M/405M [00:01<00:09, 38.6MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  13% 52.4M/405M [00:01<00:07, 49.9MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  18% 73.4M/405M [00:01<00:04, 71.6MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  23% 94.4M/405M [00:01<00:03, 88.5MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  28% 115M/405M [00:02<00:02, 98.9MB/s] \u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  34% 136M/405M [00:02<00:02, 109MB/s] \u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  39% 157M/405M [00:02<00:02, 116MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  44% 178M/405M [00:02<00:01, 121MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  49% 199M/405M [00:02<00:01, 124MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  54% 220M/405M [00:02<00:01, 123MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  60% 241M/405M [00:03<00:01, 125MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  65% 262M/405M [00:03<00:01, 127MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  70% 283M/405M [00:03<00:00, 129MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  75% 304M/405M [00:03<00:00, 125MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  80% 325M/405M [00:03<00:00, 126MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  85% 346M/405M [00:03<00:00, 127MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  91% 367M/405M [00:04<00:00, 125MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin:  96% 388M/405M [00:04<00:00, 128MB/s]\u001b[A\n",
            "Downloading (…)l-00020-of-00033.bin: 100% 405M/405M [00:04<00:00, 93.5MB/s]\n",
            "Downloading shards:  61% 20/33 [01:36<01:02,  4.84s/it]\n",
            "Downloading (…)l-00021-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:   3% 10.5M/405M [00:00<00:07, 53.3MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:   8% 31.5M/405M [00:00<00:04, 90.0MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  13% 52.4M/405M [00:00<00:03, 109MB/s] \u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  18% 73.4M/405M [00:00<00:02, 114MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  23% 94.4M/405M [00:00<00:02, 122MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  28% 115M/405M [00:01<00:02, 125MB/s] \u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  34% 136M/405M [00:01<00:02, 119MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  39% 157M/405M [00:01<00:02, 123MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  44% 178M/405M [00:01<00:01, 125MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  49% 199M/405M [00:01<00:01, 126MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  54% 220M/405M [00:01<00:01, 127MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  60% 241M/405M [00:02<00:01, 127MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  65% 262M/405M [00:02<00:01, 126MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  70% 283M/405M [00:02<00:00, 123MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  75% 304M/405M [00:02<00:00, 126MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  80% 325M/405M [00:02<00:00, 129MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  85% 346M/405M [00:02<00:00, 131MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  91% 367M/405M [00:02<00:00, 131MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin:  96% 388M/405M [00:03<00:00, 131MB/s]\u001b[A\n",
            "Downloading (…)l-00021-of-00033.bin: 100% 405M/405M [00:03<00:00, 123MB/s]\n",
            "Downloading shards:  64% 21/33 [01:40<00:53,  4.44s/it]\n",
            "Downloading (…)l-00022-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:   3% 10.5M/405M [00:00<00:07, 53.6MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:   8% 31.5M/405M [00:00<00:04, 91.1MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  13% 52.4M/405M [00:00<00:03, 108MB/s] \u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  18% 73.4M/405M [00:00<00:02, 116MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  23% 94.4M/405M [00:00<00:02, 122MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  28% 115M/405M [00:01<00:02, 126MB/s] \u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  34% 136M/405M [00:01<00:02, 127MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  39% 157M/405M [00:01<00:01, 129MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  44% 178M/405M [00:01<00:01, 130MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  49% 199M/405M [00:01<00:01, 131MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  54% 220M/405M [00:01<00:01, 132MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  60% 241M/405M [00:01<00:01, 133MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  65% 262M/405M [00:02<00:01, 133MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  70% 283M/405M [00:02<00:00, 134MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  75% 304M/405M [00:02<00:00, 134MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  80% 325M/405M [00:02<00:00, 134MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  85% 346M/405M [00:02<00:00, 134MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  91% 367M/405M [00:02<00:00, 134MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin:  96% 388M/405M [00:03<00:00, 131MB/s]\u001b[A\n",
            "Downloading (…)l-00022-of-00033.bin: 100% 405M/405M [00:03<00:00, 126MB/s]\n",
            "Downloading shards:  67% 22/33 [01:43<00:45,  4.13s/it]\n",
            "Downloading (…)l-00023-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:   3% 10.5M/405M [00:00<00:20, 18.8MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:   5% 21.0M/405M [00:00<00:13, 28.5MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:   8% 31.5M/405M [00:00<00:09, 38.8MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  10% 41.9M/405M [00:01<00:07, 50.0MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  16% 62.9M/405M [00:01<00:04, 71.9MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  21% 83.9M/405M [00:01<00:03, 89.8MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  26% 105M/405M [00:01<00:02, 103MB/s]  \u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  31% 126M/405M [00:01<00:02, 112MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  36% 147M/405M [00:01<00:02, 119MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  41% 168M/405M [00:02<00:01, 123MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  47% 189M/405M [00:02<00:01, 122MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  52% 210M/405M [00:02<00:01, 125MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  57% 231M/405M [00:02<00:01, 128MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  62% 252M/405M [00:02<00:01, 129MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  67% 273M/405M [00:02<00:01, 130MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  73% 294M/405M [00:02<00:00, 132MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  78% 315M/405M [00:03<00:00, 129MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  83% 336M/405M [00:03<00:00, 130MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  88% 357M/405M [00:03<00:00, 131MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin:  93% 377M/405M [00:03<00:00, 132MB/s]\u001b[A\n",
            "Downloading (…)l-00023-of-00033.bin: 100% 405M/405M [00:03<00:00, 105MB/s]\n",
            "Downloading shards:  70% 23/33 [01:47<00:40,  4.10s/it]\n",
            "Downloading (…)l-00024-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:   3% 10.5M/405M [00:00<00:12, 32.0MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:   5% 21.0M/405M [00:00<00:08, 44.5MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:   8% 31.5M/405M [00:00<00:06, 56.8MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  10% 41.9M/405M [00:00<00:05, 69.3MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  16% 62.9M/405M [00:00<00:03, 89.5MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  18% 73.4M/405M [00:01<00:03, 92.0MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  23% 94.4M/405M [00:01<00:02, 106MB/s] \u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  28% 115M/405M [00:01<00:02, 113MB/s] \u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  34% 136M/405M [00:01<00:02, 119MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  39% 157M/405M [00:01<00:02, 122MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  44% 178M/405M [00:01<00:01, 125MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  49% 199M/405M [00:01<00:01, 128MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  54% 220M/405M [00:02<00:01, 129MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  60% 241M/405M [00:02<00:01, 130MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  65% 262M/405M [00:02<00:01, 129MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  70% 283M/405M [00:02<00:00, 128MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  75% 304M/405M [00:02<00:00, 128MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  80% 325M/405M [00:02<00:00, 129MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  85% 346M/405M [00:03<00:00, 126MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  91% 367M/405M [00:03<00:00, 124MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin:  96% 388M/405M [00:03<00:00, 124MB/s]\u001b[A\n",
            "Downloading (…)l-00024-of-00033.bin: 100% 405M/405M [00:03<00:00, 112MB/s]\n",
            "Downloading shards:  73% 24/33 [01:51<00:36,  4.00s/it]\n",
            "Downloading (…)l-00025-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:   3% 10.5M/405M [00:00<00:17, 22.8MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:   5% 21.0M/405M [00:00<00:11, 33.1MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:   8% 31.5M/405M [00:00<00:08, 43.8MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  10% 41.9M/405M [00:00<00:06, 55.3MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  16% 62.9M/405M [00:01<00:04, 76.5MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  21% 83.9M/405M [00:01<00:03, 93.4MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  26% 105M/405M [00:01<00:02, 104MB/s]  \u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  31% 126M/405M [00:01<00:02, 110MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  36% 147M/405M [00:01<00:02, 114MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  41% 168M/405M [00:01<00:02, 117MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  47% 189M/405M [00:02<00:01, 121MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  52% 210M/405M [00:02<00:01, 121MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  57% 231M/405M [00:02<00:01, 123MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  62% 252M/405M [00:02<00:01, 125MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  67% 273M/405M [00:02<00:01, 127MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  73% 294M/405M [00:02<00:00, 129MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  78% 315M/405M [00:03<00:00, 130MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  83% 336M/405M [00:03<00:00, 131MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  88% 357M/405M [00:03<00:00, 131MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin:  93% 377M/405M [00:03<00:00, 131MB/s]\u001b[A\n",
            "Downloading (…)l-00025-of-00033.bin: 100% 405M/405M [00:03<00:00, 107MB/s]\n",
            "Downloading shards:  76% 25/33 [01:55<00:31,  3.99s/it]\n",
            "Downloading (…)l-00026-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:   3% 10.5M/405M [00:00<00:16, 24.0MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:   5% 21.0M/405M [00:00<00:11, 34.8MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:   8% 31.5M/405M [00:00<00:08, 46.0MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  10% 41.9M/405M [00:00<00:06, 57.0MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  13% 52.4M/405M [00:01<00:05, 68.2MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  18% 73.4M/405M [00:01<00:03, 90.5MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  23% 94.4M/405M [00:01<00:02, 105MB/s] \u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  28% 115M/405M [00:01<00:02, 110MB/s] \u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  34% 136M/405M [00:01<00:02, 115MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  39% 157M/405M [00:01<00:02, 119MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  44% 178M/405M [00:01<00:01, 123MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  49% 199M/405M [00:02<00:01, 124MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  54% 220M/405M [00:02<00:01, 127MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  60% 241M/405M [00:02<00:01, 128MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  65% 262M/405M [00:02<00:01, 129MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  70% 283M/405M [00:02<00:00, 130MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  75% 304M/405M [00:02<00:00, 131MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  80% 325M/405M [00:03<00:00, 132MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  85% 346M/405M [00:03<00:00, 129MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  91% 367M/405M [00:03<00:00, 123MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin:  96% 388M/405M [00:03<00:00, 127MB/s]\u001b[A\n",
            "Downloading (…)l-00026-of-00033.bin: 100% 405M/405M [00:03<00:00, 108MB/s]\n",
            "Downloading shards:  79% 26/33 [01:59<00:27,  3.98s/it]\n",
            "Downloading (…)l-00027-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:   3% 10.5M/405M [00:00<00:33, 11.6MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:   5% 21.0M/405M [00:01<00:19, 19.7MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:   8% 31.5M/405M [00:01<00:13, 28.6MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  10% 41.9M/405M [00:01<00:09, 38.8MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  16% 62.9M/405M [00:01<00:05, 59.9MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  21% 83.9M/405M [00:01<00:04, 78.6MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  26% 105M/405M [00:01<00:03, 93.0MB/s] \u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  31% 126M/405M [00:02<00:02, 104MB/s] \u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  36% 147M/405M [00:02<00:02, 112MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  41% 168M/405M [00:02<00:02, 118MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  47% 189M/405M [00:02<00:01, 123MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  52% 210M/405M [00:02<00:01, 118MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  57% 231M/405M [00:02<00:01, 122MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  62% 252M/405M [00:03<00:01, 126MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  67% 273M/405M [00:03<00:01, 127MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  73% 294M/405M [00:03<00:00, 129MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  78% 315M/405M [00:03<00:00, 131MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  83% 336M/405M [00:03<00:00, 128MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  88% 357M/405M [00:03<00:00, 130MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin:  93% 377M/405M [00:04<00:00, 131MB/s]\u001b[A\n",
            "Downloading (…)l-00027-of-00033.bin: 100% 405M/405M [00:04<00:00, 94.0MB/s]\n",
            "Downloading shards:  82% 27/33 [02:03<00:25,  4.17s/it]\n",
            "Downloading (…)l-00028-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:   3% 10.5M/405M [00:00<00:33, 11.6MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:   5% 21.0M/405M [00:01<00:19, 19.6MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:   8% 31.5M/405M [00:01<00:13, 28.5MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  10% 41.9M/405M [00:01<00:09, 38.6MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  16% 62.9M/405M [00:01<00:05, 59.5MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  21% 83.9M/405M [00:01<00:04, 78.1MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  26% 105M/405M [00:01<00:03, 92.5MB/s] \u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  31% 126M/405M [00:02<00:02, 104MB/s] \u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  36% 147M/405M [00:02<00:02, 112MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  41% 168M/405M [00:02<00:02, 118MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  47% 189M/405M [00:02<00:01, 122MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  52% 210M/405M [00:02<00:01, 125MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  57% 231M/405M [00:02<00:01, 127MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  62% 252M/405M [00:03<00:01, 129MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  67% 273M/405M [00:03<00:01, 126MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  73% 294M/405M [00:03<00:00, 128MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  78% 315M/405M [00:03<00:00, 129MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  83% 336M/405M [00:03<00:00, 128MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  88% 357M/405M [00:03<00:00, 127MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin:  93% 377M/405M [00:04<00:00, 127MB/s]\u001b[A\n",
            "Downloading (…)l-00028-of-00033.bin: 100% 405M/405M [00:04<00:00, 93.9MB/s]\n",
            "Downloading shards:  85% 28/33 [02:08<00:21,  4.36s/it]\n",
            "Downloading (…)l-00029-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:   3% 10.5M/405M [00:00<00:33, 11.6MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:   5% 21.0M/405M [00:01<00:19, 19.6MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:   8% 31.5M/405M [00:01<00:13, 28.5MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  10% 41.9M/405M [00:01<00:09, 38.7MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  13% 52.4M/405M [00:01<00:07, 49.1MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  16% 62.9M/405M [00:01<00:05, 59.1MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  21% 83.9M/405M [00:01<00:04, 78.1MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  26% 105M/405M [00:02<00:03, 90.5MB/s] \u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  31% 126M/405M [00:02<00:02, 98.8MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  36% 147M/405M [00:02<00:02, 104MB/s] \u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  41% 168M/405M [00:02<00:02, 109MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  47% 189M/405M [00:02<00:01, 112MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  52% 210M/405M [00:02<00:01, 114MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  57% 231M/405M [00:03<00:01, 115MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  62% 252M/405M [00:03<00:01, 115MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  67% 273M/405M [00:03<00:01, 114MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  73% 294M/405M [00:03<00:00, 113MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  78% 315M/405M [00:03<00:00, 118MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  83% 336M/405M [00:03<00:00, 123MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  88% 357M/405M [00:04<00:00, 126MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin:  93% 377M/405M [00:04<00:00, 128MB/s]\u001b[A\n",
            "Downloading (…)l-00029-of-00033.bin: 100% 405M/405M [00:04<00:00, 89.3MB/s]\n",
            "Downloading shards:  88% 29/33 [02:13<00:18,  4.52s/it]\n",
            "Downloading (…)l-00030-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:   3% 10.5M/405M [00:00<00:28, 14.0MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:   5% 21.0M/405M [00:01<00:16, 22.6MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:   8% 31.5M/405M [00:01<00:11, 32.1MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  10% 41.9M/405M [00:01<00:08, 42.6MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  16% 62.9M/405M [00:01<00:05, 64.2MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  21% 83.9M/405M [00:01<00:04, 79.4MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  26% 105M/405M [00:01<00:03, 92.5MB/s] \u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  31% 126M/405M [00:01<00:02, 101MB/s] \u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  36% 147M/405M [00:02<00:02, 109MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  41% 168M/405M [00:02<00:02, 113MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  47% 189M/405M [00:02<00:01, 118MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  52% 210M/405M [00:02<00:01, 117MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  57% 231M/405M [00:02<00:01, 117MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  62% 252M/405M [00:03<00:01, 120MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  67% 273M/405M [00:03<00:01, 120MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  73% 294M/405M [00:03<00:00, 122MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  78% 315M/405M [00:03<00:00, 124MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  83% 336M/405M [00:03<00:00, 125MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  88% 357M/405M [00:03<00:00, 125MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin:  93% 377M/405M [00:04<00:00, 127MB/s]\u001b[A\n",
            "Downloading (…)l-00030-of-00033.bin: 100% 405M/405M [00:04<00:00, 95.2MB/s]\n",
            "Downloading shards:  91% 30/33 [02:18<00:13,  4.53s/it]\n",
            "Downloading (…)l-00031-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:   3% 10.5M/405M [00:00<00:34, 11.5MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:   5% 21.0M/405M [00:01<00:19, 19.4MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:   8% 31.5M/405M [00:01<00:13, 28.2MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  10% 41.9M/405M [00:01<00:09, 38.2MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  13% 52.4M/405M [00:01<00:07, 48.1MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  16% 62.9M/405M [00:01<00:05, 57.6MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  21% 83.9M/405M [00:01<00:04, 77.7MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  26% 105M/405M [00:02<00:03, 92.3MB/s] \u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  31% 126M/405M [00:02<00:02, 102MB/s] \u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  36% 147M/405M [00:02<00:02, 109MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  41% 168M/405M [00:02<00:02, 111MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  47% 189M/405M [00:02<00:01, 114MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  52% 210M/405M [00:02<00:01, 110MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  57% 231M/405M [00:03<00:01, 113MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  62% 252M/405M [00:03<00:01, 115MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  67% 273M/405M [00:03<00:01, 114MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  73% 294M/405M [00:03<00:00, 118MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  78% 315M/405M [00:03<00:00, 120MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  83% 336M/405M [00:03<00:00, 122MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  88% 357M/405M [00:04<00:00, 123MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin:  93% 377M/405M [00:04<00:00, 124MB/s]\u001b[A\n",
            "Downloading (…)l-00031-of-00033.bin: 100% 405M/405M [00:04<00:00, 88.9MB/s]\n",
            "Downloading shards:  94% 31/33 [02:23<00:09,  4.64s/it]\n",
            "Downloading (…)l-00032-of-00033.bin:   0% 0.00/405M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:   3% 10.5M/405M [00:00<00:33, 11.6MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:   5% 21.0M/405M [00:01<00:19, 19.6MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:   8% 31.5M/405M [00:01<00:13, 28.5MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  10% 41.9M/405M [00:01<00:09, 38.3MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  13% 52.4M/405M [00:01<00:07, 49.0MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  18% 73.4M/405M [00:01<00:04, 70.1MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  23% 94.4M/405M [00:01<00:03, 85.0MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  28% 115M/405M [00:02<00:03, 95.7MB/s] \u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  34% 136M/405M [00:02<00:02, 102MB/s] \u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  39% 157M/405M [00:02<00:02, 107MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  44% 178M/405M [00:02<00:02, 111MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  49% 199M/405M [00:02<00:01, 112MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  54% 220M/405M [00:02<00:01, 115MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  60% 241M/405M [00:03<00:01, 115MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  65% 262M/405M [00:03<00:01, 117MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  70% 283M/405M [00:03<00:01, 118MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  75% 304M/405M [00:03<00:00, 119MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  80% 325M/405M [00:03<00:00, 116MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  85% 346M/405M [00:04<00:00, 117MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  91% 367M/405M [00:04<00:00, 117MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin:  96% 388M/405M [00:04<00:00, 118MB/s]\u001b[A\n",
            "Downloading (…)l-00032-of-00033.bin: 100% 405M/405M [00:04<00:00, 88.4MB/s]\n",
            "Downloading shards:  97% 32/33 [02:27<00:04,  4.72s/it]\n",
            "Downloading (…)l-00033-of-00033.bin:   0% 0.00/524M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:   2% 10.5M/524M [00:00<00:09, 55.0MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:   6% 31.5M/524M [00:00<00:05, 93.1MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  10% 52.4M/524M [00:00<00:04, 107MB/s] \u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  14% 73.4M/524M [00:00<00:03, 114MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  18% 94.4M/524M [00:00<00:03, 118MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  22% 115M/524M [00:01<00:03, 119MB/s] \u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  26% 136M/524M [00:01<00:03, 122MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  30% 157M/524M [00:01<00:02, 125MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  34% 178M/524M [00:01<00:02, 119MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  38% 199M/524M [00:01<00:02, 123MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  42% 220M/524M [00:01<00:02, 124MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  46% 241M/524M [00:02<00:02, 127MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  50% 262M/524M [00:02<00:02, 123MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  54% 283M/524M [00:02<00:01, 125MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  58% 304M/524M [00:02<00:01, 126MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  62% 325M/524M [00:02<00:01, 125MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  66% 346M/524M [00:02<00:01, 125MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  70% 367M/524M [00:03<00:01, 127MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  74% 388M/524M [00:03<00:01, 120MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  78% 409M/524M [00:03<00:00, 123MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  82% 430M/524M [00:03<00:00, 125MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  86% 451M/524M [00:03<00:00, 126MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  90% 472M/524M [00:03<00:00, 125MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin:  94% 493M/524M [00:04<00:00, 126MB/s]\u001b[A\n",
            "Downloading (…)l-00033-of-00033.bin: 100% 524M/524M [00:04<00:00, 121MB/s]\n",
            "Downloading shards: 100% 33/33 [02:32<00:00,  4.62s/it]\n",
            "Loading checkpoint shards: 100% 33/33 [01:48<00:00,  3.28s/it]\n",
            "Downloading (…)neration_config.json: 100% 124/124 [00:00<00:00, 16.8kB/s]\n",
            "Extended vocabulary size: 49954\n",
            "Downloading (…)/adapter_config.json: 100% 472/472 [00:00<00:00, 135kB/s]\n",
            "Downloading adapter_model.bin: 100% 858M/858M [00:13<00:00, 65.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 量化模型\n",
        "接下来我们使用[llama.cpp](https://github.com/ggerganov/llama.cpp)工具对上一步生成的全量版本权重进行转换，生成4-bit量化模型。\n",
        "\n",
        "首先对llama.cpp工具进行编译。"
      ],
      "metadata": {
        "id": "ueexcKo-Q_EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GbjsT2wRRCR",
        "outputId": "d8ec74c4-c9a0-4f95-906d-b46af0db9c8e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I llama.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -march=native -mtune=native\n",
            "I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread\n",
            "I LDFLAGS:  \n",
            "I CC:       cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "I CXX:      g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "\n",
            "cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -march=native -mtune=native   -c ggml.c -o ggml.o\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread -c llama.cpp -o llama.o\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread -c examples/common.cpp -o common.o\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/main/main.cpp ggml.o llama.o common.o -o main \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/quantize/quantize.cpp ggml.o llama.o -o quantize \n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/perplexity/perplexity.cpp ggml.o llama.o common.o -o perplexity \n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/embedding/embedding.cpp ggml.o llama.o common.o -o embedding \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "然后，我们将模型转换为ggml格式（FP16），并进一步转换为4-bit量化模型。\n",
        "- 在这之前需要把`7B-combined`目录挪个位置，并且保证符合转换脚本的要求。\n",
        "- tokenizer文件需要在模型文件的父节点上（注意使用的是LoRA权重带的，而不是转换出来的）。\n",
        "- 这里我们直接从https://huggingface.co/ziqingyang/chinese-alpaca-lora-7b/resolve/main/tokenizer.model 下载中文Alpaca-7B的tokenizer.model文件。"
      ],
      "metadata": {
        "id": "gw2xpYC0RcQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && mkdir zh-models && mv ../7B-combined zh-models/7B"
      ],
      "metadata": {
        "id": "5KgnFVStRjio"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp/zh-models && wget https://huggingface.co/ziqingyang/chinese-alpaca-lora-7b/resolve/main/tokenizer.model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cl2E2WBPSnmw",
        "outputId": "87662fff-e263-4218-a910-9ec3668005e2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-03 04:09:48--  https://huggingface.co/ziqingyang/chinese-alpaca-lora-7b/resolve/main/tokenizer.model\n",
            "Resolving huggingface.co (huggingface.co)... 54.82.45.103, 52.22.128.237, 34.206.0.154, ...\n",
            "Connecting to huggingface.co (huggingface.co)|54.82.45.103|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/0f/01/0f01544c04c27e0a0357540e7be5763000a215cedb3be4a0356b56983f2fd5e3/2d967e855b1213a439df6c8ce2791f869c84b4f3b6cfacf22b86440b8192a2f8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27tokenizer.model%3B+filename%3D%22tokenizer.model%22%3B&Expires=1680754188&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzBmLzAxLzBmMDE1NDRjMDRjMjdlMGEwMzU3NTQwZTdiZTU3NjMwMDBhMjE1Y2VkYjNiZTRhMDM1NmI1Njk4M2YyZmQ1ZTMvMmQ5NjdlODU1YjEyMTNhNDM5ZGY2YzhjZTI3OTFmODY5Yzg0YjRmM2I2Y2ZhY2YyMmI4NjQ0MGI4MTkyYTJmOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODA3NTQxODh9fX1dfQ__&Signature=iMEEUBhIUaJG2NZ3x7-TVLIHMbPVAF--XYMCPJ9R3UNq2JBaHtdpHOi3zoVGX9NZA8fCJnj0LVFJZ7ia%7EBBqKsiZMhfFUGU75nD0dR9NvgRHjGWzMIvEXfvyioePef1drz1W-bDCkZOowswQI%7E0qASxCGCyAEkkXIs8h7CViRX%7E7LGNkEYKSvrx3UI3H8NlUBV5MufHKqjinJZTyrTaXOad2W9ToAE6rXhlpdNPKpbEYs9QV%7EGWFcZyV1vHCioq28lx3%7EDhMLDLFDMXaqkShmMtmhFwAy5SvMDMe0aXpKf-WfAQn-%7EGPXUyjT5sGJZyL9ro64bN5y3GE%7E9GfkjxI4Q__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-04-03 04:09:48--  https://cdn-lfs.huggingface.co/repos/0f/01/0f01544c04c27e0a0357540e7be5763000a215cedb3be4a0356b56983f2fd5e3/2d967e855b1213a439df6c8ce2791f869c84b4f3b6cfacf22b86440b8192a2f8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27tokenizer.model%3B+filename%3D%22tokenizer.model%22%3B&Expires=1680754188&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzBmLzAxLzBmMDE1NDRjMDRjMjdlMGEwMzU3NTQwZTdiZTU3NjMwMDBhMjE1Y2VkYjNiZTRhMDM1NmI1Njk4M2YyZmQ1ZTMvMmQ5NjdlODU1YjEyMTNhNDM5ZGY2YzhjZTI3OTFmODY5Yzg0YjRmM2I2Y2ZhY2YyMmI4NjQ0MGI4MTkyYTJmOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODA3NTQxODh9fX1dfQ__&Signature=iMEEUBhIUaJG2NZ3x7-TVLIHMbPVAF--XYMCPJ9R3UNq2JBaHtdpHOi3zoVGX9NZA8fCJnj0LVFJZ7ia%7EBBqKsiZMhfFUGU75nD0dR9NvgRHjGWzMIvEXfvyioePef1drz1W-bDCkZOowswQI%7E0qASxCGCyAEkkXIs8h7CViRX%7E7LGNkEYKSvrx3UI3H8NlUBV5MufHKqjinJZTyrTaXOad2W9ToAE6rXhlpdNPKpbEYs9QV%7EGWFcZyV1vHCioq28lx3%7EDhMLDLFDMXaqkShmMtmhFwAy5SvMDMe0aXpKf-WfAQn-%7EGPXUyjT5sGJZyL9ro64bN5y3GE%7E9GfkjxI4Q__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.64.174.106, 18.64.174.110, 18.64.174.109, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.64.174.106|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 757972 (740K) [binary/octet-stream]\n",
            "Saving to: ‘tokenizer.model.1’\n",
            "\n",
            "tokenizer.model.1   100%[===================>] 740.21K  3.35MB/s    in 0.2s    \n",
            "\n",
            "2023-04-03 04:09:48 (3.35 MB/s) - ‘tokenizer.model.1’ saved [757972/757972]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && python convert-pth-to-ggml.py zh-models/7B/ 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUHeoTMQS1AQ",
        "outputId": "34a816ea-970e-4716-9947-04907b37367d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'dim': 4096, 'multiple_of': 256, 'n_heads': 32, 'n_layers': 32, 'norm_eps': 1e-06, 'vocab_size': -1}\n",
            "Namespace(dir_model='zh-models/7B/', ftype=1, vocab_only=0)\n",
            "n_parts = 1\n",
            "\n",
            "Processing part 1 of 1\n",
            "\n",
            "Processing variable: tok_embeddings.weight with shape: (49954, 4096) and type: torch.float16\n",
            "Processing variable: layers.0.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.0.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.0.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.0.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.0.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.0.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.0.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.0.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.0.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.1.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.1.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.1.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.1.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.1.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.1.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.1.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.1.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.1.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.2.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.2.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.2.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.2.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.2.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.2.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.2.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.2.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.2.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.3.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.3.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.3.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.3.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.3.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.3.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.3.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.3.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.3.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.4.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.4.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.4.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.4.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.4.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.4.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.4.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.4.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.4.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.5.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.5.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.5.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.5.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.5.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.5.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.5.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.5.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.5.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.6.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.6.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.6.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.6.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.6.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.6.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.6.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.6.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.6.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.7.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.7.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.7.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.7.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.7.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.7.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.7.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.7.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.7.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.8.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.8.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.8.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.8.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.8.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.8.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.8.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.8.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.8.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.9.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.9.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.9.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.9.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.9.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.9.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.9.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.9.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.9.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.10.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.10.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.10.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.10.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.10.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.10.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.10.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.10.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.10.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.11.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.11.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.11.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.11.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.11.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.11.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.11.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.11.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.11.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.12.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.12.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.12.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.12.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.12.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.12.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.12.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.12.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.12.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.13.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.13.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.13.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.13.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.13.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.13.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.13.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.13.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.13.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.14.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.14.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.14.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.14.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.14.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.14.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.14.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.14.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.14.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.15.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.15.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.15.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.15.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.15.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.15.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.15.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.15.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.15.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.16.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.16.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.16.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.16.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.16.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.16.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.16.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.16.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.16.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.17.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.17.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.17.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.17.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.17.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.17.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.17.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.17.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.17.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.18.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.18.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.18.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.18.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.18.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.18.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.18.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.18.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.18.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.19.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.19.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.19.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.19.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.19.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.19.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.19.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.19.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.19.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.20.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.20.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.20.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.20.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.20.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.20.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.20.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.20.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.20.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.21.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.21.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.21.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.21.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.21.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.21.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.21.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.21.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.21.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.22.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.22.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.22.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.22.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.22.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.22.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.22.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.22.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.22.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.23.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.23.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.23.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.23.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.23.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.23.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.23.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.23.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.23.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.24.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.24.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.24.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.24.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.24.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.24.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.24.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.24.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.24.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.25.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.25.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.25.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.25.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.25.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.25.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.25.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.25.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.25.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.26.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.26.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.26.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.26.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.26.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.26.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.26.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.26.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.26.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.27.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.27.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.27.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.27.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.27.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.27.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.27.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.27.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.27.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.28.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.28.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.28.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.28.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.28.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.28.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.28.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.28.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.28.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.29.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.29.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.29.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.29.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.29.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.29.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.29.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.29.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.29.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.30.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.30.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.30.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.30.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.30.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.30.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.30.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.30.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.30.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.31.attention.wq.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.31.attention.wk.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.31.attention.wv.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.31.attention.wo.weight with shape: (4096, 4096) and type: torch.float16\n",
            "Processing variable: layers.31.feed_forward.w1.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.31.feed_forward.w2.weight with shape: (4096, 11008) and type: torch.float16\n",
            "Processing variable: layers.31.feed_forward.w3.weight with shape: (11008, 4096) and type: torch.float16\n",
            "Processing variable: layers.31.attention_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: layers.31.ffn_norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: norm.weight with shape: (4096,) and type: torch.float16\n",
            "  Converting to float32\n",
            "Processing variable: output.weight with shape: (49954, 4096) and type: torch.float16\n",
            "Done. Output file: zh-models/7B//ggml-model-f16.bin\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && ./quantize ./zh-models/7B/ggml-model-f16.bin ./zh-models/7B/ggml-model-q4_0.bin 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xyais7OUVDI",
        "outputId": "ba5ca746-8412-449a-bab9-7a4feaed33d6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llama_model_quantize_internal: loading model from './zh-models/7B/ggml-model-f16.bin'\n",
            "llama_model_quantize_internal: n_vocab = 49954\n",
            "llama_model_quantize_internal: n_ctx   = 512\n",
            "llama_model_quantize_internal: n_embd  = 4096\n",
            "llama_model_quantize_internal: n_mult  = 256\n",
            "llama_model_quantize_internal: n_head  = 32\n",
            "llama_model_quantize_internal: n_layer = 32\n",
            "llama_model_quantize_internal: f16     = 1\n",
            "                           tok_embeddings.weight - [ 4096, 49954], type =    f16 quantizing .. size =   780.53 MB ->   121.96 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                    layers.0.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.021 0.016 0.028 0.046 0.071 0.103 0.137 0.158 0.137 0.103 0.071 0.046 0.028 0.016 0.021 \n",
            "                    layers.0.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.021 0.016 0.027 0.045 0.071 0.104 0.138 0.158 0.139 0.104 0.071 0.045 0.027 0.016 0.021 \n",
            "                    layers.0.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.018 0.032 0.051 0.076 0.103 0.128 0.141 0.128 0.103 0.075 0.051 0.032 0.019 0.022 \n",
            "                    layers.0.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.021 0.016 0.028 0.046 0.072 0.105 0.136 0.151 0.136 0.105 0.072 0.046 0.028 0.016 0.021 \n",
            "                 layers.0.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.0.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                 layers.0.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.125 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                  layers.0.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                        layers.0.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                    layers.1.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.032 0.051 0.077 0.104 0.127 0.137 0.127 0.104 0.077 0.051 0.032 0.019 0.022 \n",
            "                    layers.1.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.021 0.018 0.032 0.051 0.076 0.104 0.128 0.138 0.128 0.104 0.077 0.051 0.032 0.018 0.022 \n",
            "                    layers.1.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.018 0.031 0.051 0.076 0.104 0.129 0.139 0.129 0.104 0.076 0.051 0.031 0.018 0.021 \n",
            "                    layers.1.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.021 0.016 0.028 0.046 0.071 0.104 0.137 0.154 0.137 0.104 0.071 0.046 0.028 0.016 0.021 \n",
            "                 layers.1.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.1.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.1.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                  layers.1.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                        layers.1.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                    layers.2.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.032 0.052 0.077 0.104 0.126 0.135 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                    layers.2.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.032 0.051 0.076 0.104 0.127 0.137 0.127 0.104 0.077 0.051 0.032 0.019 0.022 \n",
            "                    layers.2.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.125 0.136 0.125 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                    layers.2.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.032 0.052 0.077 0.104 0.126 0.135 0.126 0.104 0.077 0.052 0.032 0.019 0.022 \n",
            "                 layers.2.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.2.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.2.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                  layers.2.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                        layers.2.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                    layers.3.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.135 0.126 0.104 0.077 0.052 0.032 0.019 0.022 \n",
            "                    layers.3.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.032 0.052 0.077 0.104 0.126 0.136 0.126 0.104 0.077 0.052 0.032 0.019 0.022 \n",
            "                    layers.3.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.125 0.135 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                    layers.3.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.3.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.3.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.3.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                  layers.3.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                        layers.3.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                    layers.4.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.135 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                    layers.4.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.032 0.052 0.077 0.104 0.126 0.135 0.126 0.104 0.077 0.052 0.032 0.019 0.022 \n",
            "                    layers.4.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.125 0.135 0.125 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                    layers.4.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.078 0.104 0.125 0.134 0.125 0.104 0.077 0.053 0.033 0.019 0.022 \n",
            "                 layers.4.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.4.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.4.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                  layers.4.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                        layers.4.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                    layers.5.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                    layers.5.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                    layers.5.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.125 0.135 0.125 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                    layers.5.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.5.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.5.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.5.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                  layers.5.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                        layers.5.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                    layers.6.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                    layers.6.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.125 0.134 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                    layers.6.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.126 0.134 0.125 0.104 0.077 0.053 0.033 0.019 0.022 \n",
            "                    layers.6.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.6.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.6.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.6.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                  layers.6.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                        layers.6.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                    layers.7.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                    layers.7.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.126 0.134 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                    layers.7.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.125 0.135 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                    layers.7.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.7.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.7.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.7.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                  layers.7.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                        layers.7.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                    layers.8.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                    layers.8.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.052 0.033 0.019 0.022 \n",
            "                    layers.8.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.125 0.134 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                    layers.8.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.8.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.8.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.078 0.052 0.033 0.019 0.022 \n",
            "                 layers.8.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                  layers.8.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                        layers.8.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                    layers.9.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                    layers.9.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.077 0.053 0.033 0.019 0.022 \n",
            "                    layers.9.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.125 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                    layers.9.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.124 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.9.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.9.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                 layers.9.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                  layers.9.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                        layers.9.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.10.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.10.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.077 0.053 0.033 0.019 0.022 \n",
            "                   layers.10.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.135 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                   layers.10.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.10.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.10.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                layers.10.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.10.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.10.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.11.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.11.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.11.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.125 0.135 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                   layers.11.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.11.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.11.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.032 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.077 0.052 0.032 0.019 0.022 \n",
            "                layers.11.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.11.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.11.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.12.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.12.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.12.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.135 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                   layers.12.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.124 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.12.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.12.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                layers.12.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.12.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.12.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.13.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.13.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.052 0.033 0.019 0.022 \n",
            "                   layers.13.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.135 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                   layers.13.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.124 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.13.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.13.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.078 0.052 0.033 0.019 0.022 \n",
            "                layers.13.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.13.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.13.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.14.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.14.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.14.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.125 0.134 0.125 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                   layers.14.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.124 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.14.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.14.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                layers.14.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.14.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.14.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.15.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.15.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.15.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.077 0.053 0.033 0.019 0.022 \n",
            "                   layers.15.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.15.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.15.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.078 0.104 0.126 0.134 0.125 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                layers.15.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.15.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.15.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.16.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.16.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.16.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.125 0.135 0.125 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                   layers.16.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.16.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.16.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.078 0.104 0.126 0.134 0.126 0.104 0.078 0.052 0.033 0.019 0.022 \n",
            "                layers.16.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.16.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.16.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.17.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.17.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.17.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.125 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                   layers.17.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.17.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.17.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.052 0.033 0.019 0.022 \n",
            "                layers.17.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.17.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.17.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.18.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.18.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.18.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.125 0.134 0.125 0.104 0.077 0.053 0.033 0.019 0.022 \n",
            "                   layers.18.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.124 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.18.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.18.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.18.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.18.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.18.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.19.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.19.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.19.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.078 0.104 0.125 0.134 0.125 0.104 0.077 0.053 0.033 0.019 0.022 \n",
            "                   layers.19.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.124 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.19.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.124 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.19.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.19.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.19.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.19.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.20.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.20.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.20.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.20.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.124 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.20.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.124 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.20.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.20.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.20.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.20.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.21.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.21.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.21.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.21.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.124 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.21.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.124 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.21.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.21.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.21.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.21.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.22.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.22.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.22.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.077 0.053 0.033 0.019 0.022 \n",
            "                   layers.22.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.124 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.22.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.22.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.22.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.22.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.22.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.23.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.23.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.125 0.134 0.125 0.104 0.077 0.053 0.033 0.019 0.022 \n",
            "                   layers.23.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.077 0.053 0.033 0.019 0.022 \n",
            "                   layers.23.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.23.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.124 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.23.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.23.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.23.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.23.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.24.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.24.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.24.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.24.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.124 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.24.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.24.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.24.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.24.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.24.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.25.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.25.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.25.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.25.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.25.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.25.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.25.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.25.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.25.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.26.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.26.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.26.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.26.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.26.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.26.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.26.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.26.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.26.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.27.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.27.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.27.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.27.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.27.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.27.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.27.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.27.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.27.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.28.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.28.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.28.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.125 0.134 0.125 0.104 0.077 0.053 0.033 0.019 0.022 \n",
            "                   layers.28.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.28.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.132 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.28.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.28.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.28.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.28.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.29.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.29.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.29.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.29.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.29.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.29.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.032 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.077 0.052 0.032 0.019 0.022 \n",
            "                layers.29.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.29.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.29.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.30.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.126 0.134 0.125 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                   layers.30.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                   layers.30.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.125 0.134 0.125 0.104 0.077 0.053 0.033 0.019 0.022 \n",
            "                   layers.30.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.30.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.30.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.018 0.032 0.051 0.076 0.104 0.128 0.137 0.128 0.104 0.076 0.051 0.032 0.018 0.022 \n",
            "                layers.30.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.30.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.30.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                   layers.31.attention.wq.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.032 0.052 0.077 0.104 0.126 0.135 0.126 0.104 0.077 0.052 0.032 0.019 0.022 \n",
            "                   layers.31.attention.wk.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "                   layers.31.attention.wv.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.032 0.052 0.077 0.104 0.126 0.135 0.126 0.104 0.077 0.052 0.032 0.019 0.022 \n",
            "                   layers.31.attention.wo.weight - [ 4096,  4096], type =    f16 quantizing .. size =    64.00 MB ->    10.00 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.31.feed_forward.w1.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.133 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                layers.31.feed_forward.w2.weight - [11008,  4096], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.021 0.018 0.031 0.050 0.075 0.104 0.130 0.140 0.130 0.104 0.075 0.050 0.031 0.018 0.021 \n",
            "                layers.31.feed_forward.w3.weight - [ 4096, 11008], type =    f16 quantizing .. size =   172.00 MB ->    26.88 MB | hist: 0.000 0.022 0.019 0.033 0.053 0.077 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "                 layers.31.attention_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                       layers.31.ffn_norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                                     norm.weight - [ 4096,     1], type =    f32 size =    0.016 MB\n",
            "                                   output.weight - [ 4096, 49954], type =    f16 quantizing .. size =   780.53 MB ->   121.96 MB | hist: 0.000 0.022 0.019 0.033 0.052 0.077 0.104 0.126 0.134 0.126 0.104 0.077 0.052 0.033 0.019 0.022 \n",
            "llama_model_quantize_internal: model size  = 26266.08 MB\n",
            "llama_model_quantize_internal: quant size  =  4104.93 MB\n",
            "llama_model_quantize_internal: hist: 0.000 0.022 0.019 0.033 0.053 0.078 0.104 0.125 0.134 0.125 0.104 0.078 0.053 0.033 0.019 0.022 \n",
            "\n",
            "main: quantize time = 186128.36 ms\n",
            "main:    total time = 186128.36 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "至此已完成了所有转换步骤。\n",
        "我们运行一条命令测试一下是否能够正常加载并进行对话。\n",
        "\n",
        "FP16和Q4量化文件存放在./llama.cpp/zh-models/7B下，可按需下载使用。"
      ],
      "metadata": {
        "id": "DLkuRAo9Vkb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && ./main -m ./zh-models/7B/ggml-model-q4_0.bin --color -f ./prompts/alpaca.txt -p \"介绍一下北京的名胜古迹\" -n 512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW-ep1BsVQtG",
        "outputId": "c00ce389-16bc-4fc8-f9e1-ea16915ed95d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: seed = 1680495616\n",
            "llama_model_load: loading model from './zh-models/7B/ggml-model-q4_0.bin' - please wait ...\n",
            "llama_model_load: n_vocab = 49954\n",
            "llama_model_load: n_ctx   = 512\n",
            "llama_model_load: n_embd  = 4096\n",
            "llama_model_load: n_mult  = 256\n",
            "llama_model_load: n_head  = 32\n",
            "llama_model_load: n_layer = 32\n",
            "llama_model_load: n_rot   = 128\n",
            "llama_model_load: f16     = 2\n",
            "llama_model_load: n_ff    = 11008\n",
            "llama_model_load: n_parts = 1\n",
            "llama_model_load: type    = 1\n",
            "llama_model_load: ggml map size = 4105.59 MB\n",
            "llama_model_load: ggml ctx size =  81.25 KB\n",
            "llama_model_load: mem required  = 5897.67 MB (+ 1026.00 MB per state)\n",
            "llama_model_load: loading tensors from './zh-models/7B/ggml-model-q4_0.bin'\n",
            "llama_model_load: model size =  4104.93 MB / num tensors = 291\n",
            "llama_init_from_file: kv self size  =  256.00 MB\n",
            "\n",
            "system_info: n_threads = 4 / 4 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
            "sampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\n",
            "generate: n_ctx = 512, n_batch = 8, n_predict = 512, n_keep = 0\n",
            "\n",
            "\n",
            "\u001b[33m 介绍一下北京的名胜古迹\u001b[0m：\n",
            " 故宫博物院：\n",
            "\n",
            "位于北京市中心，是中国古代建筑和文化遗产的代表之一。 [end of text]\n",
            "\n",
            "llama_print_timings:        load time = 12216.11 ms\n",
            "llama_print_timings:      sample time =    46.19 ms /    27 runs   (    1.71 ms per run)\n",
            "llama_print_timings: prompt eval time =  8895.55 ms /     8 tokens ( 1111.94 ms per token)\n",
            "llama_print_timings:        eval time = 15610.31 ms /    27 runs   (  578.16 ms per run)\n",
            "llama_print_timings:       total time = 27873.54 ms\n"
          ]
        }
      ]
    }
  ]
}